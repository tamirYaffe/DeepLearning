import numpy as np
from keras.datasets import mnist
import math

# part 1
# Implement the following functions, which are used to carry out the forward propagation process:


def initialize_parameters(layer_dims):
    """
    :param layer_dims: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened input, layer L is the output softmax) 
    :return: a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL). 
    """

    parameters = [None]
    for layer in range(1, len(layer_dims)):
        U = np.random.uniform(-1/math.sqrt(layer_dims[layer]*layer_dims[layer - 1]), 1/math.sqrt(layer_dims[layer]*layer_dims[layer - 1]), layer_dims[layer]*layer_dims[layer - 1])
        # W = np.random.randn(layer_dims[layer], layer_dims[layer - 1]) * np.sqrt(2/layer_dims[layer - 1])
        W = np.reshape(U,(layer_dims[layer], layer_dims[layer - 1]))
        B = np.zeros(layer_dims[layer])
        layer_dict = {'W': W, 'B': B}
        parameters.append(layer_dict)
    return parameters


def linear_forward(A, W, B):
    """
    Implement the linear part of a layer's forward propagation.

    :param A: the activations of the previous layer.
    :param W: the weight matrix of the current layer (of shape [size of current layer, size of previous layer]).
    :param B: the bias vector of the current layer (of shape [size of current layer, 1]).
    :return:
     Z – the linear component of the activation function (i.e., the value before applying the non-linear function)
     linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute) 
    """

    # Z = W * A + B
    Z = matadd(np.matmul(W, A), B)
    linear_cache = {'A': A, 'W': W, 'B': B}
    return Z, linear_cache

def matadd(mat1, mat2):
    mat3 = np.copy(mat2)
    for i in range(1, mat1.shape[1]):
        mat3 = np.vstack((mat3, mat2))
    mat3 = mat3.transpose()
    return mat1 + mat3


def softmax(Z):
    """
    :param Z: the linear component of the activation function
    :return: A – the activations of the layer, activation_cache – returns Z, which will be useful for the backpropagation
    """

    # second version to compute softmax
    exp = np.exp(Z - np.max(Z))
    A = exp / exp.sum(axis=0)
    # exp_scores = np.exp(Z)
    # A = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

    activation_cache = Z
    return A, activation_cache


def relu(Z):
    """
    :param Z: the linear component of the activation function
    :return: A – the activations of the layer, activation_cache – returns Z, which will be useful for the backpropagation
    """

    A = np.maximum(Z, 0)
    activation_cache = Z
    return A, activation_cache


def linear_activation_forward(A_prev, W, B, activation):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer.

    :param A_prev: activations of the previous layer.
    :param W: the weights matrix of the current layer.
    :param B: the bias vector of the current layer.
    :param activation: the activation function to be used (a string, either “softmax” or “relu”).
    :return:
    A – the activations of the current layer.
    cache – a joint dictionary containing both linear_cache and activation_cache.
    """

    Z, linear_cache = linear_forward(A_prev, W, B)
    if activation is "softmax":
        A, activation_cache = softmax(Z)
    elif activation is "relu":
        A, activation_cache = relu(Z)
    else:
        raise Exception("activation should be either softmax or relu")
    cache = {'linear_cache': linear_cache, 'activation_cache': activation_cache}
    return A, cache


def L_model_forward(X, parameters, use_batchnorm):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation.

    :param X: the data, numpy array of shape (input size, number of examples).
    :param parameters: the initialized W and b parameters of each layer.
    :param use_batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation.
    :return:
    AL – the last post-activation value.
    caches – a list of all the cache objects generated by the linear_forward function.
    """

    caches = [None]
    first_layer = 1
    last_layer = len(parameters) - 1

    # first layer
    A, cache = linear_activation_forward(X, parameters[first_layer]['W'], parameters[first_layer]['B'], "relu")
    caches.append(cache)
    if use_batchnorm:
        A = apply_batchnorm(A)
    # hidden layers
    for layer in range(2, last_layer):
        A, cache = linear_activation_forward(A, parameters[layer]['W'], parameters[layer]['B'], "relu")
        caches.append(cache)
        if use_batchnorm:
            A = apply_batchnorm(A)
    # last layer
    AL, cache = linear_activation_forward(A, parameters[last_layer]['W'], parameters[last_layer]['B'], "softmax")
    caches.append(cache)
    return AL, caches


def compute_cost(AL, Y):
    """
    Implement the cost function defined by equation. The requested cost function is categorical cross-entropy loss.

    :param AL: probability vector corresponding to your label predictions, shape (num_of_classes, number of examples).
    :param Y: the labels vector (i.e. the ground truth).
    :return: the cross-entropy cost
    """

    data_loss = 0
    m = len(AL[0])
    for c in range(m):  # For each element in the batch
        for r in range(len(AL)):  # For each class
            if Y[r, c] != 0:  # Positive classes
                data_loss += -np.log(AL[r, c]) * Y[r, c]  # We sum the loss per class for each element of the batch
    data_loss = -1 / m * data_loss
    return data_loss


def apply_batchnorm(A):
    """
    performs batchnorm on the received activation values of a given layer.

    :param A: the activation values of a given layer.
    :return: the normalized activation values, based on the formula learned in class.
    """

    mean = np.mean(A, axis=0)
    variance = np.mean((A - mean) ** 2, axis=0)
    float_epsilon = np.finfo(float).eps
    NA = (A - mean) * 1.0 / np.sqrt(variance + float_epsilon)
    return NA


# part 2
# Implement the following functions, which are used to carry out the backward propagation process:

def Linear_backward(dZ, cache):
    """
    Implements the linear part of the backward propagation process for a single layer.

    :param dZ: the gradient of the cost with respect to the linear output of the current layer (layer l).
    :param cache: tuple of values (A_prev, W, b) coming from the forward propagation in the current layer.
    :return:
    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev.
    dW -- Gradient of the cost with respect to W (current layer l), same shape as W.
    db -- Gradient of the cost with respect to b (current layer l), same shape as b.
    """

    dA_prev = np.matmul(cache['linear_cache']['W'].transpose(), dZ)
    dW = np.matmul(dZ, cache['linear_cache']['A'].transpose()) / len(dZ[0])
    # db = np.sum(dZ, axis=1, keepdims=True) / len(dZ[0])
    db = np.mean(dZ, axis=1)  # not sure if this is what its need to be
    return dA_prev, dW, db


def linear_activation_backward(dA, cache):
    """
    Implements the backward propagation for the LINEAR->ACTIVATION layer. The function first computes dZ and then applies the linear_backward function.

    :param dA: post activation gradient of the current layer.
    :param cache: contains both the linear cache and the activations cache.
    :return:
    dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev.
    dW – Gradient of the cost with respect to W (current layer l), same shape as W.
    db – Gradient of the cost with respect to b (current layer l), same shape as b.
    """

    # activation is always relu, softmax activation should be done only once as only the output layers uses it
    dZ = relu_backward(dA, cache['activation_cache'])
    dA_prev, dW, db = Linear_backward(dZ, cache)
    return dA_prev, dW, db


def relu_backward(dA, activation_cache):
    """
    Implements backward propagation for a ReLU unit.

    :param dA: the post-activation gradient.
    :param activation_cache: contains Z (stored during the forward propagation).
    :return: dZ – gradient of the cost with respect to Z.
    """

    Z = activation_cache
    gZ = reluDerivative(Z)
    dZ = dA * gZ
    return dZ


def reluDerivative(Z):
    """
    Computes the Relu derivative.
    
    :param Z: the linear component of the activation function.
    :return: the Relu derivative.
    """""

    return (Z > 0).astype(int)


def softmax_backward(dA, activation_cache):
    """
    Implements backward propagation for a softmax unit.

    :param dA: the post-activation gradient.
    :param activation_cache: contains Y.
    :return: dZ – gradient of the cost with respect to Z.
    """

    Y = activation_cache
    dZ = dA - Y
    return dZ


def L_model_backward(AL, Y, caches):
    """
    Implement the backward propagation process for the entire network.

    :param AL: the probabilities vector, the output of the forward propagation (L_model_forward).
    :param Y: the true labels vector (the "ground truth" - true classifications).
    :param caches: list of caches containing for each layer: a) the linear cache; b) the activation.
    :return: Grads - a dictionary with the gradients
             grads["dA" + str(l)] = ...
             grads["dW" + str(l)] = ...
             grads["db" + str(l)] = ...
    """

    Grads = {}
    layers_size = len(caches)
    last_layer = layers_size - 1

    # last layer
    # dA = -np.divide(Y, AL) + np.divide(1 - Y, 1 - AL)

    dZ = softmax_backward(AL, Y)
    dA_prev, dW, db = Linear_backward(dZ, caches[last_layer])
    update_Grads(Grads, last_layer, dA_prev, dW, db)

    # hidden layers
    for layer in reversed(range(1, last_layer)):
        dA_prev, dW, db = linear_activation_backward(dA_prev, caches[layer])
        update_Grads(Grads, layer, dA_prev, dW, db)

    return Grads


def update_Grads(Grads, layer, dA_prev, dW, db):
    """
    Update the Grads dictionary.
    :param Grads: The dictionary.
    :param layer: Number of the layer to update.
    :param dA_prev: Gradient of the cost with respect to the activation.
    :param dW: Gradient of the cost with respect to W.
    :param db: Gradient of the cost with respect to b.
    """

    Grads["dW" + str(layer)] = dW
    Grads["db" + str(layer)] = db
    Grads["dA" + str(layer - 1)] = dA_prev


def Update_parameters(parameters, grads, learning_rate):
    """
    Updates parameters using gradient descent

    :param parameters: a python dictionary containing the DNN architecture’s parameters.
    :param grads: a python dictionary containing the gradients (generated by L_model_backward).
    :param learning_rate: the learning rate used to update the parameters (the “alpha”).
    :return: parameters – the updated values of the parameters object provided as input.
    """

    for layer in range(1, len(parameters)):
        parameters[layer]["W"] = parameters[layer]["W"] - learning_rate * grads["dW" + str(layer)]
        parameters[layer]["B"] = parameters[layer]["B"] - learning_rate * grads["db" + str(layer)]
        # B_prev = parameters[layer]["B"]
        # B_grad = learning_rate * grads["db" + str(layer)]
        # B_new = B_prev - B_grad
    return parameters


# part 3
# In this section you will use the functions you created in the previous sections to train the network and produce predictions.

def L_layer_model(X, Y, learning_rate, num_iterations, batch_size, parameters):
    """
    Implements a L-layer neural network. All layers but the last should have the ReLU activation function, and the final layer will apply the softmax activation
    function. The size of the output layer should be equal to the number of labels in the data.
    Please select a batch size that enables your code to run well (i.e. no memory overflows while still running relatively fast).

    :param X: the input data, a numpy array of shape (height*width , number_of_examples).
    :param Y: the “real” labels of the data, a vector of shape (num_of_classes, number of examples).
    :param learning_rate: the learning rate.
    :param num_iterations: number of total iterations so far.
    :param batch_size: the number of examples in a single training batch.
    :param parameters: a python dictionary containing the DNN architecture’s parameters.
    :return:
     parameters – the parameters learnt by the system during the training (the same parameters that were updated in the update_parameters function).
     costs – the values of the cost function (calculated by the compute_cost function). One value is to be saved after each 100 training iterations
     (e.g. 3000 iterations -> 30 values). 
    """

    # initialize
    use_batchnorm = False
    costs = []

    # Partition the dataset into batches of a fixed size
    number_of_batchs = int(len(X[0]) / batch_size)
    X_batchs = np.array_split(X, number_of_batchs, axis=1)
    Y_batchs = np.array_split(Y, number_of_batchs, axis=1)

    for i in range(0, number_of_batchs):
        num_iterations = num_iterations+1

        X_batch = X_batchs[i]
        Y_batch = Y_batchs[i]

        # L_model_forward
        AL, caches = L_model_forward(X_batch, parameters, use_batchnorm)

        # compute_cost
        if num_iterations%100 == 0:
            cost = compute_cost(AL, Y_batch)
            print("iteration %d: cost = %f" % (num_iterations, cost))
            costs.append(cost)

        # L_model_backward
        Grads = L_model_backward(AL, Y_batch, caches)

        # update parameters
        parameters = Update_parameters(parameters, Grads, learning_rate)

    return parameters, costs


def Predict(x_train, x_valid, x_test, y_train, y_valid, y_test):
    """
    The function receives an input data and the true labels and calculates the accuracy of the trained neural network on the data.

    :param x_train: the input train data, a numpy array of shape (height*width, number_of_examples)
    :param x_valid: the input validation data, a numpy array of shape (height*width, number_of_examples)
    :param x_test: the input test data, a numpy array of shape (height*width, number_of_examples)
    :param y_train: the “real” labels of the train data, a vector of shape (num_of_classes, number of examples)
    :param y_valid: the “real” labels of the validation data, a vector of shape (num_of_classes, number of examples)
    :param y_test: the “real” labels of the test data, a vector of shape (num_of_classes, number of examples)
    :return: test_accuracy – the accuracy measure of the neural net on the provided test data (i.e. the percentage of the samples for which the correct label
     receives the hughest confidence score)
    """

    layers_dims = [784, 20, 7, 5, 10]
    learning_rate = 0.009
    num_iterations = 48
    batch_size = 1000
    epochs = 100
    prev_val_accuracy = 0
    flat_improvement_ctr = 0

    # initialize parameters
    parameters = initialize_parameters(layers_dims)

    # train the model
    for i in range(0, epochs):
        parameters, costs = L_layer_model(x_train, y_train, learning_rate, i*num_iterations, batch_size, parameters)
        # calculate val accuracy
        val_accuracy = calculate_accuracy(parameters, x_valid, y_valid)
        print("epoch %d: val_accuracy = %f" % (i+1, val_accuracy))
        if abs(val_accuracy-prev_val_accuracy) < 0.0001:
            flat_improvement_ctr =flat_improvement_ctr + 1
            if flat_improvement_ctr == 3:
                break
        else:
            flat_improvement_ctr = 0
        prev_val_accuracy = val_accuracy

    # calculate accuracy
    test_accuracy = calculate_accuracy(parameters, x_test, y_test)

    return test_accuracy


def calculate_accuracy(parameters, X, Y):
    """
    Calculate accuracy of the model given input X and Y.

    :param parameters: a python dictionary containing the DNN architecture’s parameters.
    :param X: the input data, a numpy array of shape (height*width, number_of_examples).
    :param Y: the “real” labels of the data, a vector of shape (num_of_classes, number of examples).
    :return: accuracy – the accuracy measure of the neural net on the provided data (i.e. the percentage of the samples for which the correct label
     receives the hughest confidence score)
    """

    use_batchnorm = False
    AL, caches = L_model_forward(X, parameters, use_batchnorm)
    num_of_samples = X.shape[1]
    AL = AL.transpose()
    ALMax = np.zeros_like(AL)
    ALMax[np.arange(len(AL)), AL.argmax(1)] = 1
    ALMax = ALMax.transpose()
    matrix_sum = ALMax + Y - 1
    num_of_correct_predictions = matrix_sum[matrix_sum > 0].sum()
    accuracy = num_of_correct_predictions / num_of_samples
    return accuracy


def load_dataset():
    """
    Loads the MINST dataset, divide to train validation and test sets, and perform flatten and reshape operations on it.

    :return:
     x_train: the input train data, a numpy array of shape (height*width, number_of_examples).
     x_valid: the input validation data, a numpy array of shape (height*width, number_of_examples).
     x_test: the input test data, a numpy array of shape (height*width, number_of_examples).
     y_train: the “real” labels of the train data, a vector of shape (num_of_classes, number of examples).
     y_valid: the “real” labels of the validation data, a vector of shape (num_of_classes, number of examples).
     y_test: the “real” labels of the test data, a vector of shape (num_of_classes, number of examples).
    """

    # x_test(10000,28,28), x_train(60000,28,28), y_test(10000,), y_train(60000,)
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    # flatten and reshape test data
    x_test = (x_test.reshape(10000, 784)).transpose()
    y_test_re = np.zeros((10, 10000))
    for i in range(0, 10000):
        y_test_re[y_test[i]][i] = 1
    y_test = y_test_re

    # flatten and reshape train data
    x_train = (x_train.reshape(60000, 784))
    y_train_re = np.zeros((60000, 10))
    for i in range(0, 60000):
        y_train_re[i][y_train[i]] = 1
    y_train = y_train_re

    # create random validation data
    training_idx = np.random.randint(x_train.shape[0], size=int(x_train.shape[0] * 80 / 100))
    validation_idx = np.random.randint(x_train.shape[0], size=int(x_train.shape[0] * 20 / 100))
    x_train, x_valid = x_train[training_idx, :].transpose(), x_train[validation_idx, :].transpose()
    y_train, y_valid = y_train[training_idx, :].transpose(), y_train[validation_idx, :].transpose()

    return x_train, x_valid, x_test, y_train, y_valid, y_test


def main():
    x_train, x_valid, x_test, y_train, y_valid, y_test = load_dataset()
    accuracy = Predict(x_train, x_valid, x_test, y_train, y_valid, y_test)
    print("training is done!, test accuracy: %f" %(accuracy))


if __name__ == '__main__':
    main()
